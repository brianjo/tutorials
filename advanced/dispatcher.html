


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Registering a Dispatched Operator in C++ &mdash; PyTorch Tutorials 1.7.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Hyperparameter tuning with Ray Tune" href="../beginner/hyperparameter_tuning_tutorial.html" />
    <link rel="prev" title="Autograd in C++ Frontend" href="cpp_autograd.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <div class="ecosystem-dropdown">
              <a id="dropdownMenuButton" data-toggle="ecosystem-dropdown">
                Ecosystem
              </a>
              <div class="ecosystem-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/hub"">
                  <span class=dropdown-title>Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class=dropdown-title>Tools & Libraries</span>
                  <p>Explore the ecosystem of tools and libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <div class="resources-dropdown">
              <a id="resourcesDropdownButton" data-toggle="resources-dropdown">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/resources"">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class=dropdown-title>About</span>
                  <p>Learn about PyTorch’s features and capabilities</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  1.7.0
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Tutorials" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption"><span class="caption-text">PyTorch Recipes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../recipes/recipes_index.html">See All Recipes</a></li>
</ul>
<p class="caption"><span class="caption-text">Learning PyTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/deep_learning_60min_blitz.html">Deep Learning with PyTorch: A 60 Minute Blitz</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/pytorch_with_examples.html">Learning PyTorch with Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/nn_tutorial.html">What is <cite>torch.nn</cite> <em>really</em>?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/tensorboard_tutorial.html">Visualizing Models, Data, and Training with TensorBoard</a></li>
</ul>
<p class="caption"><span class="caption-text">Image/Video</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/torchvision_tutorial.html">TorchVision Object Detection Finetuning Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/transfer_learning_tutorial.html">Transfer Learning for Computer Vision Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/fgsm_tutorial.html">Adversarial Example Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/dcgan_faces_tutorial.html">DCGAN Tutorial</a></li>
</ul>
<p class="caption"><span class="caption-text">Audio</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/audio_preprocessing_tutorial.html">Audio I/O and Pre-Processing with torchaudio</a></li>
</ul>
<p class="caption"><span class="caption-text">Text</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/transformer_tutorial.html">Sequence-to-Sequence Modeling with nn.Transformer and TorchText</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/char_rnn_classification_tutorial.html">NLP From Scratch: Classifying Names with a Character-Level RNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/char_rnn_generation_tutorial.html">NLP From Scratch: Generating Names with a Character-Level RNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html">NLP From Scratch: Translation with a Sequence to Sequence Network and Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/text_sentiment_ngrams_tutorial.html">Text Classification with TorchText</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/torchtext_translation_tutorial.html">Language Translation with TorchText</a></li>
</ul>
<p class="caption"><span class="caption-text">Reinforcement Learning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/reinforcement_q_learning.html">Reinforcement Learning (DQN) Tutorial</a></li>
</ul>
<p class="caption"><span class="caption-text">Deploying PyTorch Models in Production</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/flask_rest_api_tutorial.html">Deploying PyTorch in Python via a REST API with Flask</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/Intro_to_TorchScript_tutorial.html">Introduction to TorchScript</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_export.html">Loading a TorchScript Model in C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="super_resolution_with_onnxruntime.html">(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime</a></li>
</ul>
<p class="caption"><span class="caption-text">Frontend APIs</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../intermediate/named_tensor_tutorial.html">(prototype) Introduction to Named Tensors in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/memory_format_tutorial.html">(beta) Channels Last Memory Format in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_frontend.html">Using the PyTorch C++ Frontend</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">Custom C++ and CUDA Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_script_custom_ops.html">Extending TorchScript with Custom C++ Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_script_custom_classes.html">Extending TorchScript with Custom C++ Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch-script-parallelism.html">Dynamic Parallelism in TorchScript</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_autograd.html">Autograd in C++ Frontend</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Registering a Dispatched Operator in C++</a></li>
</ul>
<p class="caption"><span class="caption-text">Model Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/hyperparameter_tuning_tutorial.html">Hyperparameter tuning with Ray Tune</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/pruning_tutorial.html">Pruning Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="dynamic_quantization_tutorial.html">(beta) Dynamic Quantization on an LSTM Word Language Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/dynamic_quantization_bert_tutorial.html">(beta) Dynamic Quantization on BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="static_quantization_tutorial.html">(beta) Static Quantization with Eager Mode in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/quantized_transfer_learning_tutorial.html">(beta) Quantized Transfer Learning for Computer Vision Tutorial</a></li>
</ul>
<p class="caption"><span class="caption-text">Parallel and Distributed Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/dist_overview.html">PyTorch Distributed Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/model_parallel_tutorial.html">Single-Machine Model Parallel Best Practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/ddp_tutorial.html">Getting Started with Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/dist_tuto.html">Writing Distributed Applications with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/rpc_tutorial.html">Getting Started with Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/rpc_param_server_tutorial.html">Implementing a Parameter Server Using Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/dist_pipeline_parallel_tutorial.html">Distributed Pipeline Parallelism Using RPC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/rpc_async_execution.html">Implementing Batch RPC Processing Using Asynchronous Executions</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc_ddp_tutorial.html">Combining Distributed DataParallel with Distributed RPC Framework</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Tutorials
          
        </a> &gt;
      </li>

        
      <li>Registering a Dispatched Operator in C++</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/advanced/dispatcher.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <div class="pytorch-call-to-action-links">
            <div id="tutorial-type">advanced/dispatcher</div>

            <div id="google-colab-link">
              <img class="call-to-action-img" src="../_static/images/pytorch-colab.svg"/>
              <div class="call-to-action-desktop-view">Run in Google Colab</div>
              <div class="call-to-action-mobile-view">Colab</div>
            </div>
            <div id="download-notebook-link">
              <img class="call-to-action-notebook-img" src="../_static/images/pytorch-download.svg"/>
              <div class="call-to-action-desktop-view">Download Notebook</div>
              <div class="call-to-action-mobile-view">Notebook</div>
            </div>
            <div id="github-view-link">
              <img class="call-to-action-img" src="../_static/images/pytorch-github.svg"/>
              <div class="call-to-action-desktop-view">View on GitHub</div>
              <div class="call-to-action-mobile-view">GitHub</div>
            </div>
          </div>

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="registering-a-dispatched-operator-in-c">
<h1>Registering a Dispatched Operator in C++<a class="headerlink" href="#registering-a-dispatched-operator-in-c" title="Permalink to this headline">¶</a></h1>
<p>The dispatcher is an internal component of PyTorch which is responsible for
figuring out what code should actually get run when you call a function like
<code class="docutils literal notranslate"><span class="pre">torch::add</span></code>.  This can be nontrivial, because PyTorch operations need
to handle a lot of cross-cutting concerns that are “layered” on top of one
of another.  Here is a sampling of some of the things it handles:</p>
<ul class="simple">
<li>Switching between the CPU and CUDA implementations of an operator, depending
on the devices of the input tensors.</li>
<li>Switching between the autograd and backend implementations of an operator,
depending on whether or not autograd handling is necessary.</li>
<li>Applying autocasting when necessary for automatic mixed precision.</li>
<li>Applying batching rules when an operator is run under a <code class="docutils literal notranslate"><span class="pre">vmap</span></code> call.</li>
<li>Tracing execution of operations, if you are tracing a model for export.</li>
</ul>
<p>If in your <a class="reference external" href="torch_script_custom_ops">custom operator code</a> you find yourself
manually writing if statements to handle these cases, the dispatcher APIs can
help organize your code.  (Conversely, if your custom operator is very simple
and is only for CPU inference, you probably don’t need to use the dispatcher,
just use the basic API.)</p>
<p>In this tutorial, we will describe how to structure a custom operator
registration to use the dispatcher to organize various components.  We’ll
assume that you are familiar with how to
<a class="reference external" href="torch_script_custom_ops">register an operator</a> and how to write
a <a class="reference external" href="cpp_autograd">custom autograd function</a>.</p>
<div class="section" id="defining-schema-and-backend-implementations">
<h2>Defining schema and backend implementations<a class="headerlink" href="#defining-schema-and-backend-implementations" title="Permalink to this headline">¶</a></h2>
<p>The general principle behind the dispatcher is that it divides the
implementation of an operator into multiple kernels, each of which implements
functionality for a specific <em>dispatch key</em>; for example, CPU, CUDA or Autograd.
The dispatcher determines what the highest priority dispatch key is at the time
you call an operator (this is done by looking at both the tensor arguments as
well as some thread local state), and transfers control to the kernel for that
dispatch key.  The end effect is that when you call an operator, we first
execute the Autograd kernel, and then we redispatch to the CPU or CUDA kernel
depending on the device types of the passed in tensors.</p>
<p>Let’s take a look at the various parts involved in making this
happen.  First, we must define the schema for the operator in question.
Unlike simple pybind11-style operator registration, we don’t actually
provide an implementation of our operator at this point; we just
provide a schema string specifying the type signature of the operator
that all of our other kernels will abide by:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TORCH_LIBRARY</span><span class="p">(</span><span class="n">myops</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">&quot;myadd(Tensor self, Tensor other) -&gt; Tensor&quot;</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Next, we need to actually provide some implementations of this operator.
For concreteness, here is a really simple implementation of addition on CPU:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">Tensor</span> <span class="nf">myadd_cpu</span><span class="p">(</span><span class="k">const</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">self_</span><span class="p">,</span> <span class="k">const</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">other_</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">self_</span><span class="p">.</span><span class="n">sizes</span><span class="p">()</span> <span class="o">==</span> <span class="n">other_</span><span class="p">.</span><span class="n">sizes</span><span class="p">());</span>
  <span class="n">TORCH_INTERNAL_ASSERT</span><span class="p">(</span><span class="n">self_</span><span class="p">.</span><span class="n">device</span><span class="p">().</span><span class="n">type</span><span class="p">()</span> <span class="o">==</span> <span class="n">DeviceType</span><span class="o">::</span><span class="n">CPU</span><span class="p">);</span>
  <span class="n">TORCH_INTERNAL_ASSERT</span><span class="p">(</span><span class="n">other_</span><span class="p">.</span><span class="n">device</span><span class="p">().</span><span class="n">type</span><span class="p">()</span> <span class="o">==</span> <span class="n">DeviceType</span><span class="o">::</span><span class="n">CPU</span><span class="p">);</span>
  <span class="n">Tensor</span> <span class="n">self</span> <span class="o">=</span> <span class="n">self_</span><span class="p">.</span><span class="n">contiguous</span><span class="p">();</span>
  <span class="n">Tensor</span> <span class="n">other</span> <span class="o">=</span> <span class="n">other_</span><span class="p">.</span><span class="n">contiguous</span><span class="p">();</span>
  <span class="n">Tensor</span> <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">::</span><span class="n">empty</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">sizes</span><span class="p">(),</span> <span class="n">self</span><span class="p">.</span><span class="n">options</span><span class="p">());</span>
  <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">self_ptr</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>
  <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">other_ptr</span> <span class="o">=</span> <span class="n">other</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>
  <span class="kt">float</span><span class="o">*</span> <span class="n">result_ptr</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int64_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">result</span><span class="p">.</span><span class="n">numel</span><span class="p">();</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">result_ptr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">self_ptr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">other_ptr</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">result</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>We’d like to register this function as an implementation of <code class="docutils literal notranslate"><span class="pre">myops::myadd</span></code>.
However, the simple way of registering it (<code class="docutils literal notranslate"><span class="pre">def(&quot;myadd&quot;,</span> <span class="pre">myadd_cpu)</span></code>) would
register the kernel to run in all cases, even if the tensor is not a CPU
tensor!  (Internally, we refer to these as “catch-all” kernels, since they
catch all cases.)  To ensure that <code class="docutils literal notranslate"><span class="pre">myadd_cpu</span></code> is only run for
CPU tensors, we can use the <code class="docutils literal notranslate"><span class="pre">TORCH_LIBRARY_IMPL</span></code> macro:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TORCH_LIBRARY_IMPL</span><span class="p">(</span><span class="n">myops</span><span class="p">,</span> <span class="n">CPU</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">m</span><span class="p">.</span><span class="n">impl</span><span class="p">(</span><span class="s">&quot;myadd&quot;</span><span class="p">,</span> <span class="n">myadd_cpu</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">TORCH_LIBRARY_IMPL</span></code> lets us register implementations for operators on
a specific dispatch key (in this case, CPU).  Each call to <code class="docutils literal notranslate"><span class="pre">impl</span></code>
associates a CPU kernel with the corresponding operator (which we previously
defined in the <code class="docutils literal notranslate"><span class="pre">TORCH_LIBRARY</span></code> block).  If we also have a CUDA implementation <code class="docutils literal notranslate"><span class="pre">myadd_cuda</span></code>,
we can register it in a separate <code class="docutils literal notranslate"><span class="pre">TORCH_LIBRARY_IMPL</span></code> block:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TORCH_LIBRARY_IMPL</span><span class="p">(</span><span class="n">myops</span><span class="p">,</span> <span class="n">CUDA</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">m</span><span class="p">.</span><span class="n">impl</span><span class="p">(</span><span class="s">&quot;myadd&quot;</span><span class="p">,</span> <span class="n">myadd_cuda</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>These registrations can be split across files or even across library boundaries; so
for example, you could have these two <code class="docutils literal notranslate"><span class="pre">TORCH_LIBRARY_IMPL</span></code> blocks compiled
into a separate <code class="docutils literal notranslate"><span class="pre">myops_cpu</span></code> and <code class="docutils literal notranslate"><span class="pre">myops_cuda</span></code> dynamic libraries.  Generally,
speaking, the structure of your registrations will look like this:</p>
<ol class="arabic simple">
<li>A single <code class="docutils literal notranslate"><span class="pre">TORCH_LIBRARY</span></code> that lists every custom operator in your namespace
in a centralized place.</li>
<li>A <code class="docutils literal notranslate"><span class="pre">TORCH_LIBRARY_IMPL</span></code> per dispatch key that registers implementations for
that key (e.g., CPU or CUDA).  If you like, you can further subdivide
<code class="docutils literal notranslate"><span class="pre">TORCH_LIBRARY_IMPL</span></code> blocks into a block per operator. This is convenient
if you have a separate file per operator implementation, but don’t want to
expose the operators in a header; you can just put the registration in the
cpp file that defines your operator.</li>
</ol>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Did you know that you can also write <code class="docutils literal notranslate"><span class="pre">TORCH_LIBRARY_IMPL</span></code> blocks for existing
core operators in PyTorch?  This is how XLA support for PyTorch is
implemented: the <code class="docutils literal notranslate"><span class="pre">torch_xla</span></code> library contains a <code class="docutils literal notranslate"><span class="pre">TORCH_LIBRARY_IMPL</span></code>
that provides implementations for all basic operators on the XLA dispatch
key.</p>
</div>
</div>
<div class="section" id="adding-autograd-support">
<span id="autograd-support"></span><h2>Adding autograd support<a class="headerlink" href="#adding-autograd-support" title="Permalink to this headline">¶</a></h2>
<p>At this point, we have an operator with both CPU and CUDA implementations.  How
can we add autograd support to it?  As you might guess, we will register an
autograd kernel (similar to what’s described in the <a class="reference external" href="cpp_autograd">custom autograd function</a> tutorial)!
However, there is a twist: unlike the CPU and CUDA kernels, the autograd kernel
needs to <em>redispatch</em>: it needs to call back into the dispatcher to get to
the final CPU and CUDA implementations.</p>
<p>Thus, before we write the autograd kernel, let’s write a <em>dispatching function</em>
which calls into the dispatcher to find the right kernel for your operator.
This function constitutes the public C++ API for your operators–in fact, all of
the tensor functions in PyTorch’s C++ API all call the dispatcher in the same
way under the hood.  Here’s what the dispatching function looks like:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">Tensor</span> <span class="nf">myadd</span><span class="p">(</span><span class="k">const</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">self</span><span class="p">,</span> <span class="k">const</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">other</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">static</span> <span class="k">auto</span> <span class="n">op</span> <span class="o">=</span> <span class="n">torch</span><span class="o">::</span><span class="n">Dispatcher</span><span class="o">::</span><span class="n">singleton</span><span class="p">()</span>
    <span class="p">.</span><span class="n">findSchemaOrThrow</span><span class="p">(</span><span class="s">&quot;myops::myadd&quot;</span><span class="p">,</span> <span class="s">&quot;&quot;</span><span class="p">)</span>
    <span class="p">.</span><span class="n">typed</span><span class="o">&lt;</span><span class="k">decltype</span><span class="p">(</span><span class="n">myadd</span><span class="p">)</span><span class="o">&gt;</span><span class="p">();</span>
  <span class="k">return</span> <span class="n">op</span><span class="p">.</span><span class="n">call</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Let’s break it down:</p>
<ul>
<li><p class="first">In the first line, we look up a typed operator handle from the dispatcher
corresponding to the operator that we are going to dispatch to.
<code class="docutils literal notranslate"><span class="pre">findSchemaOrThrow</span></code> takes two arguments: the (namespace qualified) name
of the operator, and the overload name of the operator (typically just
the empty string).  <code class="docutils literal notranslate"><span class="pre">typed</span></code> casts the dynamically typed handle into
a statically typed handle (doing a runtime test to make sure you’ve given
the correct C++ type), so that we can do a normal C++ call on it.  We
pass it <code class="docutils literal notranslate"><span class="pre">decltype(myadd)</span></code> since the type of the dispatching function is
the same as the type of the underlying kernels registered to the dispatcher.</p>
<p>For performance, this computation is done in a static variable, so that
we only need to do the (slow) lookup once.  If you typoed the name of the
operator you want to call, this lookup will error the first time you call this
function.</p>
</li>
<li><p class="first">In the second line, we simply <code class="docutils literal notranslate"><span class="pre">call</span></code> the operator handle with all of the
arguments passed into the dispatching function.  This will actually invoke
the dispatcher and in the end control will be transferred to whatever kernel
is appropriate for this call.</p>
</li>
</ul>
<p>With the dispatch function in hand, we can now write the autograd kernel:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyAddFunction</span> <span class="o">:</span> <span class="k">public</span> <span class="n">torch</span><span class="o">::</span><span class="n">autograd</span><span class="o">::</span><span class="n">Function</span><span class="o">&lt;</span><span class="n">MyAddFunction</span><span class="o">&gt;</span> <span class="p">{</span>
 <span class="k">public</span><span class="o">:</span>
  <span class="k">static</span> <span class="n">Tensor</span> <span class="n">forward</span><span class="p">(</span>
      <span class="n">AutogradContext</span> <span class="o">*</span><span class="n">ctx</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">self</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">other</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">at</span><span class="o">::</span><span class="n">AutoNonVariableTypeMode</span> <span class="n">g</span><span class="p">;</span>
    <span class="k">return</span> <span class="nf">myadd</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="k">static</span> <span class="n">tensor_list</span> <span class="n">backward</span><span class="p">(</span><span class="n">AutogradContext</span> <span class="o">*</span><span class="n">ctx</span><span class="p">,</span> <span class="n">tensor_list</span> <span class="n">grad_outputs</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">auto</span> <span class="n">grad_output</span> <span class="o">=</span> <span class="n">grad_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
    <span class="k">return</span> <span class="p">{</span><span class="n">grad_output</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">};</span>
  <span class="p">}</span>
<span class="p">};</span>

<span class="n">Tensor</span> <span class="nf">myadd_autograd</span><span class="p">(</span><span class="k">const</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">self</span><span class="p">,</span> <span class="k">const</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">other</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">return</span> <span class="n">MyAddFunction</span><span class="o">::</span><span class="n">apply</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)[</span><span class="mi">0</span><span class="p">];</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The autograd function is written as normal using <code class="docutils literal notranslate"><span class="pre">torch::autograd::Function</span></code>,
except that instead of directly writing the implementation in <code class="docutils literal notranslate"><span class="pre">forward()</span></code>,
we:</p>
<ol class="arabic simple">
<li>Turn off autograd handling with the <code class="docutils literal notranslate"><span class="pre">at::AutoNonVariableTypeMode</span></code> RAII
guard, and then</li>
<li>Call the dispatch function <code class="docutils literal notranslate"><span class="pre">myadd</span></code> to call back into the dispatcher.</li>
</ol>
<p>Without (1), your calls will infinite loop (and stack overflow), because
<code class="docutils literal notranslate"><span class="pre">myadd</span></code> will send you back to this function (as the highest priority dispatch
key would still be autograd.) With (1),
autograd is excluded from the set of dispatch keys under consideration, and
we will go to the next handlers, which will either be CPU and CUDA.</p>
<p>We can now register this function in the same way we registered the CPU/CUDA
functions:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TORCH_LIBRARY_IMPL</span><span class="p">(</span><span class="n">myops</span><span class="p">,</span> <span class="n">Autograd</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">m</span><span class="p">.</span><span class="n">impl</span><span class="p">(</span><span class="s">&quot;myadd&quot;</span><span class="p">,</span> <span class="n">myadd_autograd</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="going-beyond-autograd">
<h2>Going beyond autograd<a class="headerlink" href="#going-beyond-autograd" title="Permalink to this headline">¶</a></h2>
<p>In some sense, the dispatcher isn’t doing all that much: all it does is
implement a glorified if-statement, along the lines of this:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyAddFunction</span> <span class="o">:</span> <span class="p">...</span> <span class="p">{</span>
<span class="k">public</span><span class="o">:</span>
  <span class="k">static</span> <span class="n">Tensor</span> <span class="n">forward</span><span class="p">(</span>
    <span class="n">AutogradContext</span> <span class="o">*</span><span class="n">ctx</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">self</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">other</span><span class="p">)</span> <span class="p">{</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="p">().</span><span class="n">type</span><span class="p">()</span> <span class="o">==</span> <span class="n">DeviceType</span><span class="o">::</span><span class="n">CPU</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">return</span> <span class="n">add_cpu</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="p">().</span><span class="n">type</span><span class="p">()</span> <span class="o">==</span> <span class="n">DeviceType</span><span class="o">::</span><span class="n">CUDA</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">return</span> <span class="n">add_cuda</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="n">TORCH_CHECK</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s">&quot;Unsupported device &quot;</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="p">().</span><span class="n">type</span><span class="p">());</span>
    <span class="p">}</span>
  <span class="p">}</span>
  <span class="p">...</span>
<span class="p">}</span>
</pre></div>
</div>
<p>So why use the dispatcher?  There are a few reasons:</p>
<ol class="arabic simple">
<li>It is decentralized.  You can assemble all of the pieces of an operator
(CPU, CUDA, Autograd) without having to write a single, centralized
if statement that refers to all of them.  Importantly, third parties can
register extra implementations for other aspects without having to patch the
original definition of an operator.</li>
<li>It supports more dispatch keys than CPU, CUDA and Autograd.  You can
see a full list of dispatch keys that are currently implemented
in PyTorch in <code class="docutils literal notranslate"><span class="pre">c10/core/DispatchKey.h</span></code>.  These dispatch keys
implement a variety of optional functionality for operators, and if you
decide you want your custom operator to support this functionality,
all you have to register a kernel for the appropriate key.</li>
<li>The dispatcher implements support for boxed fallback functions, which
are functions that can be implemented once and apply to all operators
in the system.  Boxed fallbacks can be used to provide default behavior
for a dispatch key; if you use the dispatcher to implement your operator,
you also opt into the fallbacks for all of these operations.</li>
</ol>
<p>Here are some particular dispatch keys which you may need to define an operator
for.</p>
<div class="section" id="autocast">
<h3>Autocast<a class="headerlink" href="#autocast" title="Permalink to this headline">¶</a></h3>
<p>The Autocast dispatch key implements support for
<a class="reference external" href="https://pytorch.org/docs/stable/amp.html">automatic mixed precision (AMP)</a>.
An autocast wrapper kernel typically casts incoming <code class="docutils literal notranslate"><span class="pre">float16</span></code> or <code class="docutils literal notranslate"><span class="pre">float32</span></code> CUDA tensors
to some preferred precision before running the op.
For example, matmuls and convolutions on floating-point CUDA tensors usually run faster
and use less memory in <code class="docutils literal notranslate"><span class="pre">float16</span></code> without impairing convergence.
Autocast wrappers only have an effect in
<a class="reference external" href="https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.autocast">autocast-enabled contexts</a>.</p>
<p>Here’s an autocast wrapper for a hypothetical custom matmul, along with its registration:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Autocast-specific helper functions</span>
<span class="cp">#include</span> <span class="cpf">&lt;ATen/autocast_mode.h&gt;</span><span class="cp"></span>

<span class="n">Tensor</span> <span class="nf">mymatmul_autocast</span><span class="p">(</span><span class="k">const</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">self</span><span class="p">,</span> <span class="k">const</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">other</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">c10</span><span class="o">::</span><span class="n">impl</span><span class="o">::</span><span class="n">ExcludeDispatchKeyGuard</span> <span class="n">no_autocast</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">DispatchKey</span><span class="o">::</span><span class="n">Autocast</span><span class="p">);</span>
  <span class="k">return</span> <span class="n">mymatmul</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">autocast</span><span class="o">::</span><span class="n">cached_cast</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">kHalf</span><span class="p">,</span> <span class="n">self</span><span class="p">),</span>
                  <span class="n">at</span><span class="o">::</span><span class="n">autocast</span><span class="o">::</span><span class="n">cached_cast</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">kHalf</span><span class="p">,</span> <span class="n">other</span><span class="p">));</span>
<span class="p">}</span>

<span class="n">TORCH_LIBRARY_IMPL</span><span class="p">(</span><span class="n">myops</span><span class="p">,</span> <span class="n">Autocast</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">m</span><span class="p">.</span><span class="n">impl</span><span class="p">(</span><span class="s">&quot;mymatmul&quot;</span><span class="p">,</span> <span class="n">mymatmul_autocast</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">cached_cast(kHalf,</span> <span class="pre">tensor)</span></code> casts <code class="docutils literal notranslate"><span class="pre">tensor</span></code> to <code class="docutils literal notranslate"><span class="pre">float16</span></code> if <code class="docutils literal notranslate"><span class="pre">tensor</span></code> is CUDA and <code class="docutils literal notranslate"><span class="pre">float32</span></code>,
otherwise, it leaves <code class="docutils literal notranslate"><span class="pre">tensor</span></code> unchanged (c.f. the
<a class="reference external" href="https://pytorch.org/docs/stable/amp.html#op-eligibility">eligibility policy</a> for natively autocasted ops).
This ensures if the network calls <code class="docutils literal notranslate"><span class="pre">mymatmul</span></code> on any mixture of <code class="docutils literal notranslate"><span class="pre">float16</span></code> and <code class="docutils literal notranslate"><span class="pre">float32</span></code> CUDA tensors,
<code class="docutils literal notranslate"><span class="pre">mymatmul</span></code> runs in <code class="docutils literal notranslate"><span class="pre">float16</span></code>.  Meanwhile, calls to <code class="docutils literal notranslate"><span class="pre">mymatmul</span></code> with non-CUDA, integer-type, or <code class="docutils literal notranslate"><span class="pre">float64</span></code>
inputs are unaffected.  Using <code class="docutils literal notranslate"><span class="pre">cached_cast</span></code> to follow the native eligibility policy in your own autocast wrapper
is recommended, but not required.  For example, if you wanted to force <code class="docutils literal notranslate"><span class="pre">float16</span></code> execution for all input types,
you could <code class="docutils literal notranslate"><span class="pre">return</span> <span class="pre">mymatmul(self.half(),</span> <span class="pre">other.half());</span></code> instead of using <code class="docutils literal notranslate"><span class="pre">cached_cast</span></code>.</p>
<p>Notice that, like our autograd kernels, we exclude the <code class="docutils literal notranslate"><span class="pre">Autocast</span></code> key from
dispatch before redispatching.</p>
<p>By default, if no autocast wrapper is provided,
we fallthrough directly to the regular operator implementation (no
autocasting occurs).  (We didn’t use <code class="docutils literal notranslate"><span class="pre">myadd</span></code> for this example, since pointwise
addition doesn’t need autocasting and should just fall through.)</p>
<p>When should an autocast wrapper be registered? Unfortunately, there aren’t
cut-and-dried rules for an op’s preferred precision.  You can
get a sense for some native ops’ preferred precisions by looking at the
<a class="reference external" href="https://pytorch.org/docs/master/amp.html#op-specific-behavior">cast lists</a>.
General guidance:</p>
<ul class="simple">
<li>Ops that do reductions should probably execute in <code class="docutils literal notranslate"><span class="pre">float32</span></code>,</li>
<li>Any op that does a convolution or gemm under the hood should
probably execute in <code class="docutils literal notranslate"><span class="pre">float16</span></code>, and</li>
<li>Other ops with multiple floating-point tensor inputs should standardize
them to a common precision (unless the implementation supports inputs with different precisions).</li>
</ul>
<p>If your custom op falls into the third category, the <code class="docutils literal notranslate"><span class="pre">promote_type</span></code> template
helps figure out the widest floating-point type present among input tensors, which is
the safest choice for the execution type:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span> <span class="cpf">&lt;ATen/autocast_mode.h&gt;</span><span class="cp"></span>

<span class="n">Tensor</span> <span class="nf">my_multiple_input_op_autocast</span><span class="p">(</span><span class="k">const</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">t0</span><span class="p">,</span> <span class="k">const</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">t1</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">c10</span><span class="o">::</span><span class="n">impl</span><span class="o">::</span><span class="n">ExcludeDispatchKeyGuard</span> <span class="n">no_autocast</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">DispatchKey</span><span class="o">::</span><span class="n">Autocast</span><span class="p">);</span>
  <span class="c1">// The required at::kHalf argument is an optimistic initial guess.</span>
  <span class="k">auto</span> <span class="n">exec_type</span> <span class="o">=</span> <span class="n">at</span><span class="o">::</span><span class="n">autocast</span><span class="o">::</span><span class="n">promote_type</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">kHalf</span><span class="p">,</span> <span class="n">t0</span><span class="p">,</span> <span class="n">t1</span><span class="p">);</span>
  <span class="k">return</span> <span class="n">my_multiple_input_op</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">autocast</span><span class="o">::</span><span class="n">cached_cast</span><span class="p">(</span><span class="n">exec_type</span><span class="p">,</span> <span class="n">t0</span><span class="p">),</span>
                              <span class="n">at</span><span class="o">::</span><span class="n">autocast</span><span class="o">::</span><span class="n">cached_cast</span><span class="p">(</span><span class="n">exec_type</span><span class="p">,</span> <span class="n">t1</span><span class="p">));</span>
<span class="p">}</span>
</pre></div>
</div>
<p>If your custom op is <a class="reference internal" href="#autograd-support"><span class="std std-ref">autograd-enabled</span></a>, you only need to write and register
an autocast wrapper for the same name onto which the autograd wrapper is registered.
For example, if you wanted an autocast wrapper for the <code class="docutils literal notranslate"><span class="pre">myadd</span></code> function shown
in the autograd section, all you’d need is</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">Tensor</span> <span class="nf">myadd_autocast</span><span class="p">(</span><span class="k">const</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">self</span><span class="p">,</span> <span class="k">const</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">other</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">c10</span><span class="o">::</span><span class="n">impl</span><span class="o">::</span><span class="n">ExcludeDispatchKeyGuard</span> <span class="n">no_autocast</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">DispatchKey</span><span class="o">::</span><span class="n">Autocast</span><span class="p">);</span>
  <span class="k">return</span> <span class="n">myadd</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">autocast</span><span class="o">::</span><span class="n">cached_cast</span><span class="p">(</span><span class="o">&lt;</span><span class="n">desired</span> <span class="n">dtype</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">self</span><span class="p">),</span>
               <span class="n">at</span><span class="o">::</span><span class="n">autocast</span><span class="o">::</span><span class="n">cached_cast</span><span class="p">(</span><span class="o">&lt;</span><span class="n">desired</span> <span class="n">dtype</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">other</span><span class="p">));</span>
<span class="p">}</span>

<span class="n">TORCH_LIBRARY_IMPL</span><span class="p">(</span><span class="n">myops</span><span class="p">,</span> <span class="n">Autocast</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">m</span><span class="p">.</span><span class="n">impl</span><span class="p">(</span><span class="s">&quot;myadd&quot;</span><span class="p">,</span> <span class="n">myadd_autocast</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>There are no separate gymnastics to make the backward method autocast compatible.
However, the backward method defined in your custom autograd function will run in the same
dtype as autocast sets for the forward method, so you should choose a <code class="docutils literal notranslate"><span class="pre">&lt;desired</span> <span class="pre">dtype&gt;</span></code>
suitable for both your forward and backward methods.</p>
</div>
<div class="section" id="batched">
<h3>Batched<a class="headerlink" href="#batched" title="Permalink to this headline">¶</a></h3>
<p>Batched tensors allow you to write your code in a per-example manner, and then
have them be automatically batched when run under a <code class="docutils literal notranslate"><span class="pre">vmap</span></code> invocation.  The
API for writing batching rules is currently under development, but once it is
stabilized, you can add support for <code class="docutils literal notranslate"><span class="pre">vmap</span></code> for your operators by registering
a kernel at the Batched dispatch key.</p>
</div>
<div class="section" id="tracer">
<h3>Tracer<a class="headerlink" href="#tracer" title="Permalink to this headline">¶</a></h3>
<p>The Tracer dispatch key implements support for recording invocations of operators
into a trace when you run <code class="docutils literal notranslate"><span class="pre">torch.jit.trace</span></code>.  We intend to provide a
boxed fallback that will implement tracing for arbitrary operations,
see <a class="reference external" href="https://github.com/pytorch/pytorch/issues/41478">issue #41478</a> to track
progress.</p>
</div>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../beginner/hyperparameter_tuning_tutorial.html" class="btn btn-neutral float-right" title="Hyperparameter tuning with Ray Tune" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="cpp_autograd.html" class="btn btn-neutral" title="Autograd in C++ Frontend" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr class="helpful-hr hr-top">
      <div class="helpful-container">
        <div class="helpful-question">Was this helpful?</div>
        <div class="helpful-question yes-link" data-behavior="was-this-helpful-event" data-response="yes">Yes</div>
        <div class="helpful-question no-link" data-behavior="was-this-helpful-event" data-response="no">No</div>
        <div class="was-helpful-thank-you">Thank you</div>
      </div>
    <hr class="helpful-hr hr-bottom"/>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, PyTorch.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Registering a Dispatched Operator in C++</a><ul>
<li><a class="reference internal" href="#defining-schema-and-backend-implementations">Defining schema and backend implementations</a></li>
<li><a class="reference internal" href="#adding-autograd-support">Adding autograd support</a></li>
<li><a class="reference internal" href="#going-beyond-autograd">Going beyond autograd</a><ul>
<li><a class="reference internal" href="#autocast">Autocast</a></li>
<li><a class="reference internal" href="#batched">Batched</a></li>
<li><a class="reference internal" href="#tracer">Tracer</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script type="text/javascript" src="../_static/jquery.js"></script>
         <script type="text/javascript" src="../_static/underscore.js"></script>
         <script type="text/javascript" src="../_static/doctools.js"></script>
         <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-2', 'auto');
  ga('send', 'pageview');

</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>

<script>

  window.dataLayer = window.dataLayer || [];

  function gtag(){dataLayer.push(arguments);}

  gtag('js', new Date());
  gtag('config', 'UA-117752657-2');

</script>

<script>
  !function(f,b,e,v,n,t,s)
  {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
  n.callMethod.apply(n,arguments):n.queue.push(arguments)};
  if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
  n.queue=[];t=b.createElement(e);t.async=!0;
  t.src=v;s=b.getElementsByTagName(e)[0];
  s.parentNode.insertBefore(t,s)}(window,document,'script',
  'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');

  $("[data-behavior='call-to-action-event']").on('click', function(){
    fbq('trackCustom', "Download", {
      tutorialTitle: $('h1:first').text(),
      downloadLink: this.href,
      tutorialLink: window.location.href,
      downloadTitle: $(this).attr("data-response")
    });
    ga('send', {
      hitType: 'event',
      eventCategory: 'Download',
      eventAction: 'click',
      eventLabel: $(this).attr("data-response")
    });
    gtag('event', 'click', {
      'event_category': $(this).attr("data-response"),
      'event_label': $("h1").first().text(),
      'tutorial_link': window.location.href
    });
   });

   $("[data-behavior='was-this-helpful-event']").on('click', function(){
    $(".helpful-question").hide();
    $(".was-helpful-thank-you").show();
    fbq('trackCustom', "Was this Helpful?", {
      tutorialLink: window.location.href,
      tutorialTitle: $('h1:first').text(),
      helpful: $(this).attr("data-response")
    });
    ga('send', {
      hitType: 'event',
      eventCategory: 'Was this Helpful?',
      eventAction: 'click',
      eventLabel: $(this).attr("data-response")
    });
    gtag('event', $(this).attr("data-response"), {
      'event_category': 'Was this Helpful?',
      'event_label': $("h1").first().text()
    });
   });

   if (location.pathname == "/") {
     $(".helpful-container").hide();
     $(".hr-bottom").hide();
   }
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>
<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
            <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>